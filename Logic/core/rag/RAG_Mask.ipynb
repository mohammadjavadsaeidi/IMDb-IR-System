{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "D7qkZVjtPtCt",
    "ExecuteTime": {
     "end_time": "2024-06-26T20:53:57.699595Z",
     "start_time": "2024-06-26T20:53:57.568888Z"
    }
   },
   "source": [
    "%%capture\n",
    "!pip install transformers accelerate bitsandbytes langchain langchain-community sentence-transformers faiss-gpu pandas gdown"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KU3LtV3zPtCu",
    "outputId": "b3a2df01-12a9-4afc-d353-deac5300376b",
    "ExecuteTime": {
     "end_time": "2024-06-26T20:53:57.818435Z",
     "start_time": "2024-06-26T20:53:57.700923Z"
    }
   },
   "source": [
    "!gdown --fuzzy https://drive.google.com/file/d/1Lq2zVJlN_B4kUAu4VafQ4jXMIQiAR9vI/view?usp=sharing"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: https://drive.google.com/file/d/1Lq2zVJlN_B4kUAu4VafQ4jXMIQiAR9vI/view?usp=sharing\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MUqmeYExPtCv",
    "ExecuteTime": {
     "end_time": "2024-06-26T20:53:57.821484Z",
     "start_time": "2024-06-26T20:53:57.819333Z"
    }
   },
   "source": [
    "class Config:\n",
    "    EMBEDDING_MODEL_NAME=\"thenlper/gte-base\"\n",
    "    LLM_MODEL_NAME=\"HuggingFaceH4/zephyr-7b-beta\"\n",
    "    K = 5 # top K retrieval"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sjdAj0mnPtCv",
    "ExecuteTime": {
     "end_time": "2024-06-26T20:53:58.926482Z",
     "start_time": "2024-06-26T20:53:57.823685Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json('/Users/snapp/PycharmProjects/IMDb-IR-System/Logic/core/IMDB_crawled.json')"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jYJlIaTWPtCw",
    "ExecuteTime": {
     "end_time": "2024-06-26T20:53:59.022630Z",
     "start_time": "2024-06-26T20:53:58.927226Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# preprocess your data and only store the needed data as the context window for embedding model is limited\n",
    "\n",
    "df.dropna(subset=['first_page_summary', 'genres'], inplace=True)\n",
    "\n",
    "df = df[['first_page_summary', 'genres']]\n",
    "\n",
    "df.to_csv('data/imdb.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the CSV file and vectorize the rows using HuggingFaceEmbeddings.\n",
    "Store the results using FAISS vectorstore.\n",
    "Save the vectorestore in a pickle file for future usages."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-WafXxVaPtCw",
    "outputId": "6645aae7-6325-48f1-a631-e6349362cf7f",
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-06-26T20:53:59.023339Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.vectorstores.utils import DistanceStrategy\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from faiss import IndexFlatL2\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# load the csv\n",
    "df = pd.read_csv('data/imdb.csv')\n",
    "document_texts = []\n",
    "for index, row in df.iterrows():\n",
    "    first_page_summary = row['first_page_summary']\n",
    "    genres = row['genres']\n",
    "    combined_text = f\"{first_page_summary} {genres}\"\n",
    "    document_texts.append(combined_text)\n",
    "    \n",
    "# load the embeddings model\n",
    "embeddings_model = HuggingFaceEmbeddings(model_name=Config.EMBEDDING_MODEL_NAME)\n",
    "vectors = embeddings_model.embed_documents(document_texts)\n",
    "\n",
    "def embedding_function(texts):\n",
    "    return embeddings_model.embed_documents(texts)\n",
    "\n",
    "doc_store = {i: doc for i, doc in enumerate(document_texts)}\n",
    "index_to_doc_store_id = {i: i for i in range(len(document_texts))}\n",
    "\n",
    "index = IndexFlatL2(768)\n",
    "vectors_np = np.array(vectors).astype('float32')\n",
    "\n",
    "index.add(vectors_np)\n",
    "\n",
    "# save embed the documents using the model in a vectorstore\n",
    "vectorstore = FAISS(distance_strategy=DistanceStrategy.COSINE, embedding_function=embedding_function, docstore=doc_store, index_to_docstore_id=index_to_doc_store_id, index=index)\n",
    "\n",
    "with open(\"data/vectorstore.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vectorstore, f)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "/opt/homebrew/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/opt/homebrew/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the vectorstore as a retriever."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sN1YgCMGPtCw",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "with open(\"data/vectorstore.pkl\", \"rb\") as f:\n",
    "    vectorstore = pickle.load(f)\n",
    "\n",
    "# load the retriever from the vectorstore\n",
    "retriever = vectorstore.as_retriever()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the quantized LLM."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "7126cd7d519441278252013816472b58",
      "9f28a42abc804a5e97edd39d19fac347",
      "459bb3a35c574737a695b3e2d3e609a9",
      "95d80492b2be4005a7752a84c7a84bef",
      "001ae07ad4654f36a51934fc1b42be32",
      "c7bdaba6b09e401c80c3c8a292648682",
      "da3b93f3ee5940e89c1bbc7044c10520",
      "1eef87d0268b4ac7a0960f0304113994",
      "c9691dc55fc84e819d1dc0bbc19afe87",
      "ef05a78cdd174ee987b53c114310fde3",
      "5c0022b886284914be87b6228459e325"
     ]
    },
    "id": "JM1GYo6HR2MB",
    "outputId": "729df36c-bc0f-4167-e743-d582b35fd388",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from transformers import pipeline\n",
    "\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "# load the quantization config\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_compute_dtype=torch.float16\n",
    "# )\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(Config.LLM_MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(Config.LLM_MODEL_NAME)\n",
    "\n",
    "# init the pipeline\n",
    "READER_LLM = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=-1,\n",
    "    max_length=50,  \n",
    "    max_new_tokens=50,\n",
    "    truncation=True \n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(\n",
    "    pipeline=READER_LLM,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialize the prompt template for the query chain. query chain is used to get a query from the chat history. you may change the prompt as you like to get better results."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1eM1zuNUgMPq",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import TransformChain, LLMChain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "class LoggerStrOutputParser(StrOutputParser):\n",
    "    def parse(self, text: str) -> str:\n",
    "        # process the LLM output\n",
    "        processed_text = text.strip().split(\"\\n\")[0] \n",
    "        print(f\"QUERY: {processed_text}\")\n",
    "        return processed_text\n",
    "\n",
    "query_transform_prompt = PromptTemplate(\n",
    "    input_variables=[\"messages\"],\n",
    "    template=\"\"\"<|system|>You are a helpful assistant.\n",
    "{messages}\n",
    "<|user|>\n",
    "give me the search query about the above conversation.\n",
    "<|assistant|>\"\"\"\n",
    ")\n",
    "\n",
    "# init the query chain\n",
    "\n",
    "query_transform_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=query_transform_prompt,\n",
    "    output_parser=LoggerStrOutputParser()\n",
    ")\n",
    "\n",
    "# Function to transform the input and output keys\n",
    "def transform_function(inputs):\n",
    "    return query_transform_chain(inputs)\n",
    "\n",
    "# TransformChain to handle the conversion of chat history to search query\n",
    "query_transforming_retriever_chain = TransformChain(\n",
    "    transform=transform_function,\n",
    "    input_variables=[\"messages\"],\n",
    "    output_variables=[\"query\"]\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialize the main retrieval chain that gives the resulting documents to LLM and gets the output back."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KwMTLauLS7m0",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"messages\"],\n",
    "    template=\"\"\"<|system|>You are a helpful assistant.\n",
    "\n",
    "Here are the movies you MUST choose from:\n",
    "\n",
    "{context}\n",
    "-----------------\n",
    "{messages}\n",
    "<|assistant|>\"\"\")\n",
    "\n",
    "# init the retriver chain\n",
    "retrieval_chain = create_stuff_documents_chain(\n",
    "    prompt=prompt,\n",
    "    llm=llm,\n",
    "    output_parser=LoggerStrOutputParser(),\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write the conversation helper class for easier testing."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zhttNbN0U0WF",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "class Conversation:\n",
    "    def __init__(self, query_chain, retrieval_chain):\n",
    "        self.messages = []\n",
    "        self.query_chain = query_chain\n",
    "        self.retrieval_chain = retrieval_chain\n",
    "\n",
    "    def add_assistant_message(self, message):\n",
    "        self.messages.append(('assistant', message))\n",
    "\n",
    "    def add_user_message(self, message):\n",
    "        self.messages.append(('user', message))\n",
    "\n",
    "    def get_messages(self):\n",
    "        # concatenate the messages with the roles in the instruction format\n",
    "        formatted_messages = \"\\n\".join(\n",
    "            [f\"{role}: {msg}\" for role, msg in self.messages]\n",
    "        )\n",
    "        return formatted_messages\n",
    "\n",
    "    def chat(self, message):\n",
    "        self.add_user_message(message)\n",
    "        messages = self.get_messages()\n",
    "        # invoke the chain\n",
    "        query = self.query_chain.run({\"messages\": messages})\n",
    "        \n",
    "        # Retrieve relevant documents using the retrieval chain\n",
    "        context = self.retrieval_chain.run({\"query\": query})\n",
    "        \n",
    "        # Formulate the response based on the retrieved context and the conversation\n",
    "        response = f\"Here are some suggestions based on your query:\\n{context}\"\n",
    "        \n",
    "        self.add_assistant_message(response)\n",
    "        return response"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "talk with the RAG to see how good it performs."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g8XV3HvhVTNu",
    "outputId": "b96ea4b1-8d98-4a03-9e67-a41c7a2b3829",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "c = Conversation(query_transforming_retriever_chain, retrieval_chain)\n",
    "A = c.chat('give me a cool gangster movie')\n",
    "print(A)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cC0Sz0Nbl_Zr",
    "outputId": "092b4108-3ace-4261-ca52-108828ec3e3b",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "A = c.chat('give me a newer one')\n",
    "print(A)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "001ae07ad4654f36a51934fc1b42be32": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1eef87d0268b4ac7a0960f0304113994": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "459bb3a35c574737a695b3e2d3e609a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1eef87d0268b4ac7a0960f0304113994",
      "max": 8,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c9691dc55fc84e819d1dc0bbc19afe87",
      "value": 8
     }
    },
    "5c0022b886284914be87b6228459e325": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7126cd7d519441278252013816472b58": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9f28a42abc804a5e97edd39d19fac347",
       "IPY_MODEL_459bb3a35c574737a695b3e2d3e609a9",
       "IPY_MODEL_95d80492b2be4005a7752a84c7a84bef"
      ],
      "layout": "IPY_MODEL_001ae07ad4654f36a51934fc1b42be32"
     }
    },
    "95d80492b2be4005a7752a84c7a84bef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ef05a78cdd174ee987b53c114310fde3",
      "placeholder": "​",
      "style": "IPY_MODEL_5c0022b886284914be87b6228459e325",
      "value": " 8/8 [01:28&lt;00:00, 10.13s/it]"
     }
    },
    "9f28a42abc804a5e97edd39d19fac347": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c7bdaba6b09e401c80c3c8a292648682",
      "placeholder": "​",
      "style": "IPY_MODEL_da3b93f3ee5940e89c1bbc7044c10520",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "c7bdaba6b09e401c80c3c8a292648682": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c9691dc55fc84e819d1dc0bbc19afe87": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "da3b93f3ee5940e89c1bbc7044c10520": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ef05a78cdd174ee987b53c114310fde3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
